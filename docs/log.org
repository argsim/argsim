* structure

- embedding
- encoder
  + gru
  + attention
- latent
  + affine in
  + affine mu
  + affine lv
  + affine ex
- decoder
  + gru
  + affine out
- logit

* dropout

applying dropout always made the model worse,
dropout may not be suitable for vaes anyways,
since it forces the model to learn a distributed representation,
whereas with vaes we want a disentangled representation.

* word dropout

randomly replacing words for the decoder with =unk= forces the model to use information provided by the encoder.
crippling the decoder is desirable at the beginning,
however evetually how well the encoder learns is determined by the decoder,
since in a vae the encoder is the inference network mirroring of the decoder.

currently we an adaptive word keep rate, from 50% to 100% by the logistic curve.

we tried a variant where instead of =unk=, random words are picked.
this resulted in lower training accuracy and higher loss,
but the autoregressively generated sentences seem better.
random swapping may be a remedy for teacher forcing training.
however since using the decoder autoregressively is not our priority,
and the adaptive dropout rate is eventually zero,
we are not considering this option for now.

* input output embedding sharing

=logit_use_embed=

the idea came from [[https://arxiv.org/abs/1608.05859][press & wolf]].

simply using the transposed embedding matrix as the logit weights is detrimental.
the inputs and ouputs are not entirely symmetric,
specifically the outputs are softmax proboabilities and the inputs are not.

however when we scaled the input embedding by =sqrt dim_emb= to be used as the output embedding,
as used by [[https://arxiv.org/abs/1706.03762][vaswani et al.]] in the transformer,
it does make the model train slightly better.

* initialization

random initialization seems to play a surprisingly important role for this model.

currently all biases are initialized to zeros;
the embedding is initialized with =uniform(-0.5,0.5)=;
the affine and recurrent layers are initialized according to [[http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf][glorot & bengio]]
(=variance scaling 1.0 fan_avg=, tensorflow default);
and the relu layers with =variance scaling 2.0 fan_avg= according to [[https://arxiv.org/abs/1502.01852][he et al.]]

the idea behind variance scaling is to keep the variance stable;
=fan_in= keeps it stable for the forward pass;
=fan_out= keeps it stable for the backward pass;
=fan_avg= makes a trade off.

when we changed the relu init to glorot,
the results are similar at first,
but becomes noticably worse after 20k training steps,
presumably due to having more relu units dying out.

* relu layers

adding relu layers always made the model worse.
the cause is unclear.

changing decoder and latent at the same time always makes it difficult to judge.
decoder getting more powerful is not necessarily good,
but we do want latent to get better.

todo
- add relu layers to latent
- if that works, add relu to decode
- if that works, try relu rnn with attention

* attention

=attentive=

we are using unidirectional gru rnns, even in the encoder.
the reason for not using a bidirectional one is that the =cudnn= implementations
do not support variable lengthed sequences at the moment,
but it is so much faster than the alternatives.
for unidirectional rnns, this is not a problem, not for grus anyways,
since we can just extract the states from the outputs.
however bidirectional ones won't behave correctly.

the unidirectional encoder was a lot worse than the bidirectional one, until we added attention.
specifically, the final states after the first padding steps are used as queries,
and the outputs from all non-padding steps are used as values.
the summarized values are now the encoder outputs.
attention significantly improved the model.

we are currently experimenting with adding a unidirectional gru
which runs on the reversed sequences padded with =bos=.
the backward final states are used to query the forward outputs,
and the forward final states the backward outputs.
