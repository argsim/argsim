* structure

- embedding
- encoder
  + gru
  + attention
- latent
  + affine in
  + affine mu
  + affine lv
  + affine ex
- decoder
  + gru
  + affine out
- logit

* dropout

applying dropout always made the model worse,
dropout may not be suitable for vaes anyways,
since it forces the model to learn a distributed representation,
whereas with vaes we want a disentangled representation.

* word dropout

randomly replacing words for the decoder with =unk= forces the model to use information provided by the encoder.
crippling the decoder is desirable at the beginning,
however evetually how well the encoder learns is determined by the decoder,
since in a vae the encoder is the inference network mirroring of the decoder.

without word dropout, the accuracy is better,
even though we disable word dropout during validation.
however is autoregressively generated results are far worse.

currently we an adaptive word keep rate, from 50% to 100% by the logistic curve.

we tried a variant where instead of =unk=, random words are picked.
this resulted in lower training accuracy and higher loss,
but the autoregressively generated sentences seem better.
random swapping may be a remedy for teacher forcing training.
however since using the decoder autoregressively is not our priority,
and the adaptive dropout rate is eventually zero,
we are not considering this option for now.

* input output embedding sharing

=logit_use_embed=

the idea came from [[https://arxiv.org/abs/1608.05859][press & wolf]].

simply using the transposed embedding matrix as the logit weights is detrimental.
the inputs and ouputs are not entirely symmetric,
specifically the outputs are softmax proboabilities and the inputs are not.

however when we scaled the input embedding by =sqrt dim_emb= to be used as the output embedding,
as used by [[https://arxiv.org/abs/1706.03762][vaswani et al.]] in the transformer,
it does make the model train slightly better.

* initialization

random initialization seems to play a surprisingly important role for this model.

currently all biases are initialized to zeros;
the affine and recurrent layers are initialized according to [[http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf][glorot & bengio]]
(=variance scaling 1.0 fan_avg=, tensorflow default);
and the relu layers with =variance scaling 2.0 fan_avg= according to [[https://arxiv.org/abs/1502.01852][he et al.]]

the idea behind variance scaling is to keep the variance stable;
=fan_in= keeps it stable for the forward pass;
=fan_out= keeps it stable for the backward pass;
=fan_avg= makes a trade off.

the embedding is uniformly initialized with the bound =sqrt(6/(dim_tgt/dim_emb+1))=,
which when scaled and used as the logit weights,
is equivalent to having the logit layer initialized the default way.

when we changed the relu init to glorot,
the results are similar at first,
but becomes noticably worse after 20k training steps,
presumably due to having more relu units dying out.

* relu layers

adding relu layers always made the model worse.
the cause is unclear.

changing decoder and latent at the same time always makes it difficult to judge.
decoder getting more powerful is not necessarily good,
but we do want latent to get better.

todo
- add relu layers to latent
- if that works, add relu to decode
- if that works, try relu rnn with attention

* stacked bidirectional grus

the =cudnn= implementations do not support variable lengthed sequences at the moment,
but it is so much faster than the alternatives.
for unidirectional rnns, this is not a problem, not for grus anyways,
since we can just extract the states from the outputs.
however bidirectional ones won't behave correctly,
unless we manually reverse the sequences properly.

=bidirectional= runs two unidirectional rnns in parallel;
=bidir_stacked= stacks them the usual way.

* attention

=attentive=

we can use the final states as query, and as values the outputs from all non-padding steps.
with multihead scaled dot-product attention,
this made the model learn faster at the beginning,
especially when =dim_emb= is small,
however eventually the model without attention learned better,
even though we added residual connection around attention.
