* structure

- embedding
- encoder
  + gru
  + attention
- latent
  + affine in
  + affine mu
  + affine lv
  + affine ex
- decoder
  + gru
  + affine out
- logit

* dropout

applying dropout always made the model worse,
dropout may not be suitable for vaes anyways,
since it forces the model to learn a distributed representation,
whereas with vaes we want a disentangled representation.

* word dropout

=drop_word=

currently we use 50% word dropout for decoder,
where the inputs are randomly replaced with =unk=.

consider these alternatives:
- randomly replace inputs
- randomly set embedded inputs to zeros (a variant of dropout)

* input output embedding sharing

=logit_use_embed=

the idea came from [[https://arxiv.org/abs/1608.05859][press & wolf]].
however it always made our model worse.
it's not necessary for a model using sentencepiece anyways.

* initialization

random initialization seems to play a surprisingly important role for this model.

currently all biases are initialized to zeros;
the embedding is initialized with =uniform(-0.5,0.5)=;
the affine and recurrent layers are initialized according to [[http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf][glorot & bengio]]
(=variance scaling 1.0 fan_avg=, tensorflow default);
and the relu layers with =variance scaling 2.0 fan_avg= according to [[https://arxiv.org/abs/1502.01852][he et al.]]

the idea behind variance scaling is to keep the variance stable;
=fan_in= keeps it stable for the forward pass;
=fan_out= keeps it stable for the backward pass;
=fan_avg= makes a trade off.

when we changed the relu init to glorot,
the results are similar at first,
but becomes noticably worse after 20k training steps,
presumably due to having more relu units dying out.

* relu layers

adding relu layers always made the model worse.
the cause is unclear.

changing decoder and latent at the same time always makes it difficult to judge.
decoder getting more powerful is not necessarily good,
but we do want latent to get better.

todo
- add relu layers to latent
- if that works, add relu to decode
- if that works, try relu rnn with attention

* attention

=attentive=

we are using unidirectional gru rnns, even in the encoder.
the reason for not using a bidirectional one is that the =cudnn= implementations
do not support variable lengthed sequences at the moment,
but it is so much faster than the alternatives.
for unidirectional rnns, this is not a problem, not for grus anyways,
since we can just extract the states from the outputs.
however bidirectional ones won't behave correctly.

the unidirectional encoder was a lot worse than the bidirectional one, until we added attention.
specifically, the final states after the first padding steps are used as queries,
and the outputs from all non-padding steps are used as values.
the summarized values are now the encoder outputs.
attention significantly improved the model.

we are currently experimenting with adding a unidirectional gru
which runs on the reversed sequences padded with =bos=.
the backward final states are used to query the forward outputs,
and the forward final states the backward outputs.
