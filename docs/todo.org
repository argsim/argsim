* relu activation

- change all activations to relu
- use more dense layers
- use he init

* dropout

the vae currently does not learn when dropout is applied.
figure out why and if dropout is beneficial

* hyperparameter tuning

- word dropout rate
- model size

* attentional encoder

- bidirectional rnn
- final state as query
- outputs as values

* convolutional decoder

since we don't need to use the decoder for generation,
it does not have to be generative/autoregressive.
consider a transposed convolutional network.
