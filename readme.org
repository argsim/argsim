* argsim

variational autoencoder for argument representation learning

** documentations

- [[docs/paper/paper.pdf][the paper]]
- [[docs/report/report.pdf][the interim report]]
- [[docs/pitch/pitch.pdf][the pitch]]
- [[docs/log.org][experiment log]]
- [[docs/results_iac][post-related results]]

** quick start

*** dependencies

- [[https://www.python.org/][python3]]
- [[https://www.tensorflow.org/][tensorflow]]
- [[https://github.com/google/sentencepiece][sentencepiece]]

*** datasets

#+BEGIN_SRC bash :eval no
mkdir data
#+END_SRC

download and unzip the following datasets into =data=

- [[http://www.hlt.utdallas.edu/~saidul/stance/reason.html][reason identification and classification]] for evaluation
- [[http://www.research.ibm.com/haifa/dept/vst/debating_data.shtml#Project][claim sentences search]] for sentence modeling (use dirname =ibm_claim=)
- [[https://nlds.soe.ucsc.edu/iac][internet argument corpus v1.1]] for post modeling (use dirname =iac_v1.1=)

*** to reproduce our results

#+BEGIN_SRC bash :eval no
mkdir trial trial/data trial/ckpt
cd src
#+END_SRC

- prepare the data

run =./data_ibm.py= or =./data_iac.py=

- train a new model

#+BEGIN_SRC bash :eval no
./train.py
#+END_SRC

check =./train.py --help= for arguments
and see =config.json= for more configurations.

- evaluation and exploration

this part is messy, due to the experimental nature of this project.

basically first run =eval_prep_data_*.py= to prepare the evaluation data.
then run =eval_embed*.py= to embed the prepared data using the trained model.
finally run =eval_classification*.py= for classification
and =eval_clustering*.py= for clustering.
there are also a few scripts named =explore*.py= for explorations.

*** default paths, git ignored

- =data=: for storing raw data
- =trial/data= for storing processed data
- =trial/ckpt= for model checkpoints
- =trial/log= for tensorboard summaries
